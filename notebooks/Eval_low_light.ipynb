{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code intended to evaluate the performances of Orbslam3 and DSO using custom dataset in TUM format.\n",
    "Requirements.\n",
    "1) Container containing DSO (custom)\n",
    "2) Container containing ORBSLAM3 (custom)\n",
    "3) Folder containing the datasets\n",
    "4) Evo tools ;D\n",
    "'''\n",
    "import docker\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config path: /home/croc/Documenti/Repositories/evoTools/notebooks/config\n",
      "\t -Docker config loaded;\n",
      "\t -Local config loaded;\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "root_path = os.getcwd()\n",
    "config_path = os.path.join(root_path, \"config\")\n",
    "docker_config = os.path.join(config_path, \"docker_containers.yaml\")\n",
    "local_config = os.path.join(config_path, \"local_host_params.yaml\")\n",
    "print(\"Current config path: {}\".format(config_path))\n",
    "try:\n",
    "    with open(docker_config, \"r\") as yamlfile:\n",
    "        docker_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t -Docker config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", docker_config)\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    with open(local_config, \"r\") as yamlfile:\n",
    "        local_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t -Local config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", local_config)\n",
    "    sys.exit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container test_orbslam3 Already started\n",
      "Container test_dso Already started\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# DOCKER SET-UP\n",
    "######################################################################\n",
    "# Get the configurations for the containers\n",
    "dso_container_config = docker_data_cfg[\"Containers\"][0][\"DSO\"]\n",
    "orb_container_config = docker_data_cfg[\"Containers\"][1][\"ORBSLAM3\"]\n",
    "\n",
    "# Get the client\n",
    "client_dk = docker.from_env()\n",
    "\n",
    "# Retrieve the containers\n",
    "orb_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == orb_container_config[\"Id\"]][0]\n",
    "dso_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == dso_container_config[\"Id\"]][0]\n",
    "\n",
    "# Start the two containers\n",
    "if orb_container.status == 'exited':\n",
    "    orb_container.start()\n",
    "    print(\"Container {} Started\".format(orb_container.name))\n",
    "elif orb_container.status ==\"running\":\n",
    "     print(\"Container {} Already started\".format(orb_container.name))\n",
    "\n",
    "if dso_container.status == 'exited':\n",
    "    dso_container.start()\n",
    "    print(\"Container {} Started\".format(dso_container.name))\n",
    "elif dso_container.status ==\"running\":\n",
    "    print(\"Container {} Already started\".format(dso_container.name))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DATASET LOADING\n",
    "######################################################################\n",
    "\n",
    "#Common Parameters\n",
    "datasets_root_path = local_data_cfg[\"Datasets\"][\"root_path\"]\n",
    "datasets_map_folder = local_data_cfg[\"Datasets\"][\"map_folder\"]\n",
    "datasets_data_folder = local_data_cfg[\"Datasets\"][\"data_folder\"]\n",
    "datasets_eva_folder = local_data_cfg[\"Datasets\"][\"out_eva_folder\"]\n",
    "\n",
    "#Datasets selected\n",
    "dataset_list = local_data_cfg[\"Datasets\"][\"Scenarios\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering subsets of Corridor_A: ['Corridor_A_D_190', 'Corridor_A_D_60', 'Corridor_A_D_255', 'Corridor_A_D_85', 'Corridor_A_D_127', 'Corridor_A_L', 'Corridor_A_D_25']\n"
     ]
    }
   ],
   "source": [
    "# Sub-Datasets Discovering\n",
    "dataset_selected = list(dataset_list.keys())[0]\n",
    "folder_dataset_sel=dataset_list[dataset_selected][\"folder_name\"]\n",
    "path_dataset = os.path.join(datasets_root_path, folder_dataset_sel)\n",
    "if os.path.isdir(path_dataset):\n",
    "    elem = [elem for elem in os.listdir(os.path.join(path_dataset, datasets_data_folder))]\n",
    "    print(\"Discovering subsets of {}: {}\".format(dataset_selected,elem))\n",
    "else:\n",
    "    elem = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing '/home/croc/Documenti/Trajectories/excasi/Corridor_A_L' ...\n",
      "Compressing\n"
     ]
    }
   ],
   "source": [
    "# We'll try to process just the CorridorA\n",
    "curr_dataset = elem[5]\n",
    "path_sub_dataset = os.path.join(path_dataset, curr_dataset)\n",
    "print(\"Processing '{}' ...\".format(path_sub_dataset))\n",
    "\n",
    "print(\"Compressing\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
