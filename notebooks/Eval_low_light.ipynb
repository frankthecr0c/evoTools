{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code intended to evaluate the performances of Orbslam3 and DSO using custom dataset in TUM format.\n",
    "Requirements.\n",
    "1) Container containing DSO (custom)\n",
    "2) Container containing ORBSLAM3 (custom)\n",
    "3) Folder containing the datasets\n",
    "4) Evo tools ;D\n",
    "'''\n",
    "import docker\n",
    "from docker.errors import APIError\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "def copy_to_container(container, container_dest_path, dataset_dir):\n",
    "    print(\"Copying '{}' in \\\"{}\\\" \".format(dataset_dir,container_dest_path))\n",
    "    temp_dir_processing = os.path.join(os.getcwd(), \"temp\",\"\".join([\"curr_dataset\",\".tar.gz\"]))\n",
    "\n",
    "    print(\"\\t - 1) Compressing {}\".format(temp_dir_processing))\n",
    "    with tarfile.open(temp_dir_processing, \"w:gz\") as tar:\n",
    "        tar.add(dataset_dir, arcname=os.path.basename(dataset_dir))\n",
    "    print(\"\\t\\tDone!\")\n",
    "\n",
    "    print(\"\\t - 2) Copying into DSO containter: {}\".format(container.name))\n",
    "    with open(temp_dir_processing, 'rb') as fd:\n",
    "        ok = dso_container.put_archive(path=container_dest_path, data=fd)\n",
    "        if not ok:\n",
    "            raise Exception('Put file failed')\n",
    "        else:\n",
    "            print(\"\\t\\tDone!\")\n",
    "    if os.path.exists(temp_dir_processing):\n",
    "        print(\"\\t - 3) Removing the temp file:{}\".format(temp_dir_processing))\n",
    "        os.remove(temp_dir_processing)\n",
    "        print(\"\\t\\tDone!\")\n",
    "    else:\n",
    "        print('Unable to remove {} \\n this not a critucal problem, proceed...'.format(temp_dir_processing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config path: /home/croc/Documenti/Repositories/evoTools/notebooks/config\n",
      "\t-Docker config loaded;\n",
      "\t-Local config loaded;\n",
      "DSO CONTAINTER CONFIGURATIONS\n",
      "\t{'Id': '7a0bedaae163', 'Names': 'test_dso', 'datasetFolder': '/root/Archive/Dataset/', 'Algorithm': {'binPath': '/root/Programs/dso/build/bin/', 'exec': 'dso_dataset_params', 'output_file': 'result.txt', 'configs': {'files': 'images.zip', 'calib': 'camera.txt', 'gamma': 'pcalib.txt', 'vignette': 'vignette.png'}, 'params': {'ImmatureDensity': 400, 'PointDensity': 800, 'minFrames': 5, 'maxFrames': 7, 'maxOptIterations': 6, 'minOptIterations': 1, 'speed': 0, 'mode': 0, 'nogui': 1, 'quiet': 1}}}\n",
      "ORBSLAM CONTAINTER CONFIGURATIONS\n",
      "\t{'Id': '4ecad379b081', 'Names': 'test_orbslam3', 'datasetFolder': '/root/Archive/Dataset/', 'Algorithm': {'binPath': '/root/Programs/ORB_SLAM3/Examples/Monocular/'}}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# LOAD CONFIG\n",
    "######################################################################\n",
    "sess_root_path = os.getcwd()\n",
    "config_path = os.path.join(sess_root_path, \"config\")\n",
    "docker_config = os.path.join(config_path, \"docker_containers.yaml\")\n",
    "local_config = os.path.join(config_path, \"local_host_params.yaml\")\n",
    "print(\"Current config path: {}\".format(config_path))\n",
    "try:\n",
    "    with open(docker_config, \"r\") as yamlfile:\n",
    "        docker_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t-Docker config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", docker_config)\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    with open(local_config, \"r\") as yamlfile:\n",
    "        local_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t-Local config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", local_config)\n",
    "    sys.exit()\n",
    "\n",
    "# Get the configurations for the containers\n",
    "dso_container_config = docker_data_cfg[\"Containers\"][0][\"DSO\"]\n",
    "orb_container_config = docker_data_cfg[\"Containers\"][1][\"ORBSLAM3\"]\n",
    "\n",
    "print(\"DSO CONTAINTER CONFIGURATIONS\")\n",
    "print(\"\\t{}\".format(dso_container_config))\n",
    "\n",
    "print(\"ORBSLAM CONTAINTER CONFIGURATIONS\")\n",
    "print(\"\\t{}\".format(orb_container_config))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Session folder: /home/croc/Documenti/Repositories/evoTools/notebooks/session_02_10_2023_18_06_51\n",
      "\tDone!\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# CREATE SESSION FOLDER\n",
    "######################################################################\n",
    "# datetime object containing current date and time\n",
    "\n",
    "now = datetime.now()\n",
    "sess_string = ''.join([\"session_\",now.strftime(\"%d_%m_%Y_%H_%M_%S\")])\n",
    "session_folder = os.path.join(sess_root_path, sess_string)\n",
    "print(\"Creating Session folder: {}\".format(session_folder))\n",
    "try:\n",
    "    os.mkdir(session_folder)\n",
    "    print(\"\\tDone!\")\n",
    "except OSError as error:\n",
    "    print(\"Fatal Error: {}\".format(error))\n",
    "    sys.exit(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DOCKER SET-UP\n",
    "######################################################################\n",
    "\n",
    "# Get the client\n",
    "client_dk = docker.from_env()\n",
    "# Retrieve the containers\n",
    "orb_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == orb_container_config[\"Id\"]][0]\n",
    "dso_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == dso_container_config[\"Id\"]][0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DATASET LOADING\n",
    "######################################################################\n",
    "\n",
    "#Common Parameters\n",
    "datasets_root_path = local_data_cfg[\"Datasets\"][\"root_path\"]\n",
    "datasets_map_folder = local_data_cfg[\"Datasets\"][\"map_folder\"]\n",
    "datasets_data_folder = local_data_cfg[\"Datasets\"][\"data_folder\"]\n",
    "datasets_eva_folder = local_data_cfg[\"Datasets\"][\"out_eva_folder\"]\n",
    "dataset_gt_suffix = local_data_cfg[\"Datasets\"][\"GtFileSuffix\"].split(\"*\")[-1]\n",
    "#Datasets selected\n",
    "dataset_list = local_data_cfg[\"Datasets\"][\"Scenarios\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering subsets of Corridor_A: ['Corridor_A_D_190', 'Corridor_A_D_60', 'Corridor_A_D_255', 'Corridor_A_D_85', 'Corridor_A_D_127', 'Corridor_A_L', 'Corridor_A_D_25']\n"
     ]
    }
   ],
   "source": [
    "# Sub-Datasets Discovering\n",
    "dataset_selected = list(dataset_list.keys())[0]\n",
    "folder_dataset_sel=dataset_list[dataset_selected][\"folder_name\"]\n",
    "path_dataset = os.path.join(datasets_root_path, folder_dataset_sel)\n",
    "if os.path.isdir(path_dataset):\n",
    "    elem = [elem for elem in os.listdir(os.path.join(path_dataset, datasets_data_folder))]\n",
    "    print(\"Discovering subsets of {}: {}\".format(dataset_selected,elem))\n",
    "else:\n",
    "    elem = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      " Processing with DSO the Corridor_A_L dataset\n",
      "**************************************************\n",
      "\n",
      "Creating Dataset folder: /home/croc/Documenti/Repositories/evoTools/notebooks/session_02_10_2023_18_06_51/Corridor_A_L\n",
      "\tDone!\n",
      "Copying the gt file /home/croc/Documenti/Trajectories/excasi/Datasets/TUM Format/Corridor_A_L/Corridor_A_L_GT.txt ----> /home/croc/Documenti/Repositories/evoTools/notebooks/session_02_10_2023_18_06_51/Corridor_A_L\n",
      "\tDone!\n",
      "Copying '/home/croc/Documenti/Trajectories/excasi/Datasets/TUM Format/Corridor_A_L' in \"/root/Archive/Dataset/\" \n",
      "\t - 1) Compressing /home/croc/Documenti/Repositories/evoTools/notebooks/temp/curr_dataset.tar.gz\n",
      "\t\tDone!\n",
      "\t - 2) Copying into DSO containter: test_dso\n",
      "\t\tDone!\n",
      "\t - 3) Removing the temp file:/home/croc/Documenti/Repositories/evoTools/notebooks/temp/curr_dataset.tar.gz\n",
      "\t\tDone!\n",
      "\n",
      "On container \"test_dso\" execute the following command:  \n",
      "\tbash -c \"cd /root/Programs/dso/build/bin/ && ./dso_dataset_params files=/root/Archive/Dataset/Corridor_A_L/images.zip calib=/root/Archive/Dataset/Corridor_A_L/camera.txt gamma=/root/Archive/Dataset/Corridor_A_L/pcalib.txt vignette=/root/Archive/Dataset/Corridor_A_L/vignette.png ImmatureDensity=400 PointDensity=800 minFrames=5 maxFrames=7 maxOptIterations=6 minOptIterations=1 speed=0 mode=0 nogui=1 quiet=1\" \n",
      "\t Done!\n",
      "\n",
      "Removing /root/Archive/Dataset/Corridor_A_L from test_dso\n",
      "\t Done!\n",
      "\n",
      "Renaming the results, using the command: bash -c \"cd /root/Programs/dso/build/bin/ && mv result.txt Corridor_A_L_DSO.txt\"\n",
      "\tDone!\n",
      "\n",
      "Transferring the file /root/Programs/dso/build/bin/Corridor_A_L_DSO.txt ----> /home/croc/Documenti/Repositories/evoTools/notebooks/session_02_10_2023_18_06_51/Corridor_A_L/Corridor_A_L_DSO.txt\n",
      "None\n",
      "\tDone!\n",
      "\tStats:{'name': 'Corridor_A_L_DSO.txt', 'size': 46201, 'mode': 420, 'mtime': '2023-10-02T18:07:47.476228893+02:00', 'linkTarget': ''}\n",
      "\n",
      "Removing the following file from remote container: \t\n",
      "- /root/Programs/dso/build/bin/Corridor_A_L_DSO.txt \t\n",
      "- /root/Programs/dso/build/bin/Corridor_A_L_DSO.txt\n",
      "\tDone!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TODO: This must be extended to ORBSLAM and for all the subsets\n",
    "# We'll try to process just the CorridorA\n",
    "curr_dataset = elem[5]\n",
    "algorithm = \"DSO\"\n",
    "print(\"\\n\"+ \"*\"*50 + \"\\n Processing with {} the {} dataset\".format(algorithm, curr_dataset) +\"\\n\"+ \"*\"*50+\"\\n\")\n",
    "\n",
    "# Compose the full path of the desired Dataset\n",
    "path_sub_dataset = os.path.join(path_dataset, datasets_data_folder, curr_dataset)\n",
    "\n",
    "# Create the dataset folder results and copy the gt file\n",
    "dataset_out_folder= os.path.join(session_folder, curr_dataset)\n",
    "try:\n",
    "    print(\"Creating Dataset folder: {}\".format(dataset_out_folder))\n",
    "    os.mkdir(dataset_out_folder)\n",
    "    print(\"\\tDone!\")\n",
    "    gt_filename = \"\".join([curr_dataset, dataset_gt_suffix])\n",
    "    file_gt = os.path.join(path_sub_dataset,gt_filename)\n",
    "    print(\"Copying the gt file {} ----> {}\".format(file_gt,dataset_out_folder))\n",
    "    shutil.copy(file_gt, dataset_out_folder)\n",
    "    print(\"\\tDone!\")\n",
    "except OSError as error:\n",
    "    print(\"Fatal Error: {}\".format(error))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get the destination directory\n",
    "destination_path = dso_container_config[\"datasetFolder\"]\n",
    "\n",
    "##### END COMMON CODE, NOW IT WILL REFER JUST TO DSO#############\n",
    "# Transfer the dataset into the container\n",
    "copy_to_container(dso_container, destination_path, path_sub_dataset)\n",
    "container_actual_data = os.path.join(destination_path, curr_dataset)\n",
    "\n",
    "\n",
    "# Preparing the cmd for DSO:\n",
    "dso_container_config_algorithm = dso_container_config[\"Algorithm\"]\n",
    "dso_bin_path = dso_container_config_algorithm[\"binPath\"]\n",
    "\n",
    "# 1) Change directory command\n",
    "dso_ch_dir_string=\"\".join([\"cd\", \" \", dso_bin_path])\n",
    "\n",
    "# 2a) Executable command\n",
    "executable_string = str(os.path.join(\".\",  dso_container_config_algorithm[\"exec\"]))\n",
    "\n",
    "# 2b) Configs command\n",
    "config_string = \"\"\n",
    "first = True\n",
    "for config in dso_container_config_algorithm[\"configs\"]:\n",
    "    config_value=dso_container_config_algorithm[\"configs\"][config]\n",
    "    if first:\n",
    "        config_string+=\"\".join([str(config),\"=\",str(os.path.join(container_actual_data,config_value))])\n",
    "        first = False\n",
    "    else:\n",
    "        config_string+=\"\".join([\" \",str(config),\"=\",str(os.path.join(container_actual_data,config_value))])\n",
    "\n",
    "# 2c) Parameters command\n",
    "param_string = \"\"\n",
    "first = True\n",
    "for param in dso_container_config_algorithm[\"params\"]:\n",
    "    param_value=dso_container_config_algorithm[\"params\"][param]\n",
    "    if first:\n",
    "        param_string+=\"\".join([str(param),\"=\",str(param_value)])\n",
    "        first = False\n",
    "    else:\n",
    "        param_string+=\"\".join([\" \",str(param),\"=\",str(param_value)])\n",
    "\n",
    "# 2d) Command String\n",
    "dso_command_string = \"\".join([executable_string, \" \", config_string, \" \", param_string])\n",
    "\n",
    "\n",
    "# 3)  command string\n",
    "command_string = \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \",  dso_command_string, \"\\\"\"])\n",
    "print(\"\\nOn container \\\"{}\\\" execute the following command:  \\n\\t{} \".format(dso_container.name, command_string))\n",
    "\n",
    "# 5) Execute command\n",
    "command_res=dso_container.exec_run(command_string, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\t Done!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "\n",
    "# 6) Remove the datasets from the container\n",
    "print(\"\\nRemoving {} from {}\".format(container_actual_data,dso_container.name))\n",
    "rm_dataset_cmd = \"\".join([\"bash -c \\\"\",\"rm -r\", \" \", str(container_actual_data),\"\\\"\"])\n",
    "command_res = dso_container.exec_run(rm_dataset_cmd, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\t Done!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "\n",
    "# 7) Rename the result (a), Compress (b) and copy back to the session folder (c). (d) Finally remove results from container\n",
    "\n",
    "# (a)\n",
    "result_filename_remote_ext = \"\".join([curr_dataset, \"_\", \"DSO.txt\"])\n",
    "cmd_ren= \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \", \"mv\", \" \", dso_container_config_algorithm[\"output_file\"], \" \", result_filename_remote_ext, \"\\\"\"])\n",
    "print(\"\\nRenaming the results, using the command: {}\".format(cmd_ren))\n",
    "command_res = dso_container.exec_run(cmd_ren, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\tDone!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "# (c)\n",
    "result_cmpr_local = os.path.join(dataset_out_folder, result_filename_remote_ext)\n",
    "result_filename_remote_cmpr = os.path.join(dso_bin_path, result_filename_remote_ext)\n",
    "\n",
    "from io import BytesIO\n",
    "file_obj = BytesIO()\n",
    "print(\"\\nTransferring the file {} ----> {}\".format(result_filename_remote_cmpr, result_cmpr_local))\n",
    "try:\n",
    "    bits, stat = dso_container.get_archive(result_filename_remote_cmpr)\n",
    "    for chunk in bits:\n",
    "        file_obj.write(chunk)\n",
    "    file_obj.seek(0)\n",
    "    tar = tarfile.open(mode='r', fileobj=file_obj)\n",
    "    file = tar.extractfile(result_filename_remote_ext)\n",
    "    print(file.name)\n",
    "    with open (result_cmpr_local, \"wb\") as outfile:\n",
    "        outfile.write(file.read())\n",
    "\n",
    "except Exception as error:\n",
    "    print(\"\\tFatal Error: {}\".format(error))\n",
    "    sys.exit(1)\n",
    "print(\"\\tDone!\\n\\tStats:{}\".format(stat))\n",
    "\n",
    "#\n",
    "# (d)\n",
    "result_filename_remote = os.path.join(dso_bin_path, result_filename_remote_ext)\n",
    "remove_remote_res_cmd = \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \", \"rm\", \" \", result_filename_remote, \"\\\"\"])\n",
    "print(\"\\nRemoving the following file from remote container: \\t\\n- {} \\t\\n- {}\".format(result_filename_remote, result_filename_remote_cmpr))\n",
    "command_res = dso_container.exec_run(remove_remote_res_cmd, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\tDone!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "data": {
      "text/plain": "'/root/Programs/dso/build/bin/Corridor_A_L_DSO.txt'"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_filename_remote"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
