{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Code intended to evaluate the performances of Orbslam3 and DSO using custom dataset in TUM format.\n",
    "Requirements.\n",
    "1) Container containing DSO (custom)\n",
    "2) Container containing ORBSLAM3 (custom)\n",
    "3) Folder containing the datasets\n",
    "4) Evo tools ;D\n",
    "'''\n",
    "import docker\n",
    "from docker.errors import APIError\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def copy_to_container(container, container_dest_path, dataset_dir):\n",
    "    print(\"Copying '{}' in \\\"{}\\\" \".format(dataset_dir,container_dest_path))\n",
    "    temp_dir_processing = os.path.join(os.getcwd(), \"temp\",\"\".join([\"curr_dataset\",\".tar.gz\"]))\n",
    "\n",
    "    print(\"\\t - 1) Compressing {}\".format(temp_dir_processing))\n",
    "    with tarfile.open(temp_dir_processing, \"w:gz\") as tar:\n",
    "        tar.add(dataset_dir, arcname=os.path.basename(dataset_dir))\n",
    "    print(\"\\t\\tDone!\")\n",
    "\n",
    "    print(\"\\t - 2) Copying into DSO containter: {}\".format(container.name))\n",
    "    with open(temp_dir_processing, 'rb') as fd:\n",
    "        ok = dso_container.put_archive(path=container_dest_path, data=fd)\n",
    "        if not ok:\n",
    "            raise Exception('Put file failed')\n",
    "        else:\n",
    "            print(\"\\t\\tDone!\")\n",
    "    if os.path.exists(temp_dir_processing):\n",
    "        print(\"\\t - 3) Removing the temp file:{}\".format(temp_dir_processing))\n",
    "        os.remove(temp_dir_processing)\n",
    "        print(\"\\t\\tDone!\")\n",
    "    else:\n",
    "        print('Unable to remove {} \\n this not a critucal problem, proceed...'.format(temp_dir_processing))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config path: /home/croc/Documenti/Repositories/evoTools/notebooks/config\n",
      "\t-Docker config loaded;\n",
      "\t-Local config loaded;\n",
      "DSO CONTAINTER CONFIGURATIONS\n",
      "\t{'Id': '7a0bedaae163', 'Names': 'test_dso', 'datasetFolder': '/root/Archive/Dataset/', 'Algorithm': {'binPath': '/root/Programs/dso/build/bin/', 'exec': 'dso_dataset_params', 'output_file': 'result.txt', 'configs': {'files': 'images.zip', 'calib': 'camera.txt', 'gamma': 'pcalib.txt', 'vignette': 'vignette.png'}, 'params': {'ImmatureDensity': 400, 'PointDensity': 800, 'minFrames': 5, 'maxFrames': 7, 'maxOptIterations': 6, 'minOptIterations': 1, 'speed': 0, 'mode': 0, 'nogui': 1, 'quiet': 1}}}\n",
      "ORBSLAM CONTAINTER CONFIGURATIONS\n",
      "\t{'Id': '4ecad379b081', 'Names': 'test_orbslam3', 'datasetFolder': '/root/Archive/Dataset/', 'Algorithm': {'binPath': '/root/Programs/ORB_SLAM3/Examples/Monocular/'}}\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# LOAD CONFIG\n",
    "######################################################################\n",
    "sess_root_path = os.getcwd()\n",
    "config_path = os.path.join(sess_root_path, \"config\")\n",
    "docker_config = os.path.join(config_path, \"docker_containers.yaml\")\n",
    "local_config = os.path.join(config_path, \"local_host_params.yaml\")\n",
    "print(\"Current config path: {}\".format(config_path))\n",
    "try:\n",
    "    with open(docker_config, \"r\") as yamlfile:\n",
    "        docker_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t-Docker config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", docker_config)\n",
    "    sys.exit()\n",
    "\n",
    "try:\n",
    "    with open(local_config, \"r\") as yamlfile:\n",
    "        local_data_cfg = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "        print(\"\\t-Local config loaded;\")\n",
    "except OSError:\n",
    "    print(\"Could not open/read file:\", local_config)\n",
    "    sys.exit()\n",
    "\n",
    "# Get the configurations for the containers\n",
    "dso_container_config = docker_data_cfg[\"Containers\"][0][\"DSO\"]\n",
    "orb_container_config = docker_data_cfg[\"Containers\"][1][\"ORBSLAM3\"]\n",
    "\n",
    "print(\"DSO CONTAINTER CONFIGURATIONS\")\n",
    "print(\"\\t{}\".format(dso_container_config))\n",
    "\n",
    "print(\"ORBSLAM CONTAINTER CONFIGURATIONS\")\n",
    "print(\"\\t{}\".format(orb_container_config))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Session folder: /home/croc/Documenti/Repositories/evoTools/notebooks/session_29_09_2023_15_43_34\n",
      "\tDone!\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# CREATE SESSION FOLDER\n",
    "######################################################################\n",
    "# datetime object containing current date and time\n",
    "\n",
    "now = datetime.now()\n",
    "sess_string = ''.join([\"session_\",now.strftime(\"%d_%m_%Y_%H_%M_%S\")])\n",
    "session_folder = os.path.join(sess_root_path, sess_string)\n",
    "print(\"Creating Session folder: {}\".format(session_folder))\n",
    "try:\n",
    "    os.mkdir(session_folder)\n",
    "    print(\"\\tDone!\")\n",
    "except OSError as error:\n",
    "    print(\"Fatal Error: {}\".format(error))\n",
    "    sys.exit(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DOCKER SET-UP\n",
    "######################################################################\n",
    "\n",
    "# Get the client\n",
    "client_dk = docker.from_env()\n",
    "\n",
    "# Retrieve the containers\n",
    "orb_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == orb_container_config[\"Id\"]][0]\n",
    "dso_container = [cont for cont in client_dk.containers.list(all=True) if cont.short_id == dso_container_config[\"Id\"]][0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container test_orbslam3 Already started\n",
      "Container test_dso Already started\n"
     ]
    }
   ],
   "source": [
    "# Start the two containers\n",
    "if orb_container.status == 'exited':\n",
    "    orb_container.start()\n",
    "    print(\"Container {} Started\".format(orb_container.name))\n",
    "elif orb_container.status ==\"running\":\n",
    "     print(\"Container {} Already started\".format(orb_container.name))\n",
    "\n",
    "if dso_container.status == 'exited':\n",
    "    dso_container.start()\n",
    "    print(\"Container {} Started\".format(dso_container.name))\n",
    "elif dso_container.status ==\"running\":\n",
    "    print(\"Container {} Already started\".format(dso_container.name))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# DATASET LOADING\n",
    "######################################################################\n",
    "\n",
    "#Common Parameters\n",
    "datasets_root_path = local_data_cfg[\"Datasets\"][\"root_path\"]\n",
    "datasets_map_folder = local_data_cfg[\"Datasets\"][\"map_folder\"]\n",
    "datasets_data_folder = local_data_cfg[\"Datasets\"][\"data_folder\"]\n",
    "datasets_eva_folder = local_data_cfg[\"Datasets\"][\"out_eva_folder\"]\n",
    "dataset_gt_suffix = local_data_cfg[\"Datasets\"][\"GtFileSuffix\"].split(\"*\")[-1]\n",
    "#Datasets selected\n",
    "dataset_list = local_data_cfg[\"Datasets\"][\"Scenarios\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovering subsets of Corridor_A: ['Corridor_A_D_190', 'Corridor_A_D_60', 'Corridor_A_D_255', 'Corridor_A_D_85', 'Corridor_A_D_127', 'Corridor_A_L', 'Corridor_A_D_25']\n"
     ]
    }
   ],
   "source": [
    "# Sub-Datasets Discovering\n",
    "dataset_selected = list(dataset_list.keys())[0]\n",
    "folder_dataset_sel=dataset_list[dataset_selected][\"folder_name\"]\n",
    "path_dataset = os.path.join(datasets_root_path, folder_dataset_sel)\n",
    "if os.path.isdir(path_dataset):\n",
    "    elem = [elem for elem in os.listdir(os.path.join(path_dataset, datasets_data_folder))]\n",
    "    print(\"Discovering subsets of {}: {}\".format(dataset_selected,elem))\n",
    "else:\n",
    "    elem = []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      " Processing with DSO the Corridor_A_L dataset\n",
      "**************************************************\n",
      "\n",
      "Creating Dataset folder: /home/croc/Documenti/Repositories/evoTools/notebooks/session_29_09_2023_15_43_34/Corridor_A_L\n",
      "\tDone!\n",
      "Copying the gt file /home/croc/Documenti/Trajectories/excasi/Datasets/TUM Format/Corridor_A_L/Corridor_A_L_GT.txt ----> /home/croc/Documenti/Repositories/evoTools/notebooks/session_29_09_2023_15_43_34/Corridor_A_L\n",
      "\tDone!\n",
      "Copying '/home/croc/Documenti/Trajectories/excasi/Datasets/TUM Format/Corridor_A_L' in \"/root/Archive/Dataset/\" \n",
      "\t - 1) Compressing /home/croc/Documenti/Repositories/evoTools/notebooks/temp/curr_dataset.tar.gz\n",
      "\t\tDone!\n",
      "\t - 2) Copying into DSO containter: test_dso\n",
      "\t\tDone!\n",
      "\t - 3) Removing the temp file:/home/croc/Documenti/Repositories/evoTools/notebooks/temp/curr_dataset.tar.gz\n",
      "\t\tDone!\n",
      "\n",
      "On container \"test_dso\" execute the following command:  \n",
      "\tbash -c \"cd /root/Programs/dso/build/bin/ && ./dso_dataset_params files=/root/Archive/Dataset/Corridor_A_L/images.zip calib=/root/Archive/Dataset/Corridor_A_L/camera.txt gamma=/root/Archive/Dataset/Corridor_A_L/pcalib.txt vignette=/root/Archive/Dataset/Corridor_A_L/vignette.png ImmatureDensity=400 PointDensity=800 minFrames=5 maxFrames=7 maxOptIterations=6 minOptIterations=1 speed=0 mode=0 nogui=1 quiet=1\" \n",
      "\t Done!\n",
      "\n",
      "Removing /root/Archive/Dataset/Corridor_A_L from test_dso\n",
      "\t Done!\n",
      "\n",
      "Renaming the results, using the command: bash -c \"cd /root/Programs/dso/build/bin/ && mv result.txt Corridor_A_L_DSO.txt\"\n",
      "\tDone!\n",
      "\n",
      "compressing the output result command: bash -c \"cd /root/Programs/dso/build/bin/ && tar -cvzf Corridor_A_L_DSO.txt.tar.gz Corridor_A_L_DSO.txt\"\n",
      "\tDone!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "embedded null byte",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[54], line 122\u001B[0m\n\u001B[1;32m    120\u001B[0m strm,status \u001B[38;5;241m=\u001B[39m dso_container\u001B[38;5;241m.\u001B[39mget_archive(result_filename_remote_cmpr)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m strm:\n\u001B[0;32m--> 122\u001B[0m    pw_tar \u001B[38;5;241m=\u001B[39m \u001B[43mtarfile\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTarFile\u001B[49m\u001B[43m(\u001B[49m\u001B[43md\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    123\u001B[0m    pw_tar\u001B[38;5;241m.\u001B[39mextractall(dataset_out_folder)\n",
      "File \u001B[0;32m~/anaconda3/envs/evoTools/lib/python3.8/tarfile.py:1646\u001B[0m, in \u001B[0;36mTarFile.__init__\u001B[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001B[0m\n\u001B[1;32m   1644\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1645\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1646\u001B[0m     fileobj \u001B[38;5;241m=\u001B[39m \u001B[43mbltn_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extfileobj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1648\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: embedded null byte"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TODO: This must be extended to ORBSLAM and for all the subsets\n",
    "# We'll try to process just the CorridorA\n",
    "curr_dataset = elem[5]\n",
    "algorithm = \"DSO\"\n",
    "print(\"\\n\"+ \"*\"*50 + \"\\n Processing with {} the {} dataset\".format(algorithm, curr_dataset) +\"\\n\"+ \"*\"*50+\"\\n\")\n",
    "\n",
    "# Compose the full path of the desired Dataset\n",
    "path_sub_dataset = os.path.join(path_dataset, datasets_data_folder, curr_dataset)\n",
    "\n",
    "# Create the dataset folder results and copy the gt file\n",
    "dataset_out_folder= os.path.join(session_folder, curr_dataset)\n",
    "try:\n",
    "    print(\"Creating Dataset folder: {}\".format(dataset_out_folder))\n",
    "    os.mkdir(dataset_out_folder)\n",
    "    print(\"\\tDone!\")\n",
    "    gt_filename = \"\".join([curr_dataset, dataset_gt_suffix])\n",
    "    file_gt = os.path.join(path_sub_dataset,gt_filename)\n",
    "    print(\"Copying the gt file {} ----> {}\".format(file_gt,dataset_out_folder))\n",
    "    shutil.copy(file_gt, dataset_out_folder)\n",
    "    print(\"\\tDone!\")\n",
    "except OSError as error:\n",
    "    print(\"Fatal Error: {}\".format(error))\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get the destination directory\n",
    "destination_path = dso_container_config[\"datasetFolder\"]\n",
    "\n",
    "##### END COMMON CODE, NOW IT WILL REFER JUST TO DSO#############\n",
    "# Transfer the dataset into the container\n",
    "copy_to_container(dso_container, destination_path, path_sub_dataset)\n",
    "container_actual_data = os.path.join(destination_path, curr_dataset)\n",
    "\n",
    "\n",
    "# Preparing the cmd for DSO:\n",
    "dso_container_config_algorithm = dso_container_config[\"Algorithm\"]\n",
    "dso_bin_path = dso_container_config_algorithm[\"binPath\"]\n",
    "\n",
    "# 1) Change directory command\n",
    "dso_ch_dir_string=\"\".join([\"cd\", \" \", dso_bin_path])\n",
    "\n",
    "# 2a) Executable command\n",
    "executable_string = str(os.path.join(\".\",  dso_container_config_algorithm[\"exec\"]))\n",
    "\n",
    "# 2b) Configs command\n",
    "config_string = \"\"\n",
    "first = True\n",
    "for config in dso_container_config_algorithm[\"configs\"]:\n",
    "    config_value=dso_container_config_algorithm[\"configs\"][config]\n",
    "    if first:\n",
    "        config_string+=\"\".join([str(config),\"=\",str(os.path.join(container_actual_data,config_value))])\n",
    "        first = False\n",
    "    else:\n",
    "        config_string+=\"\".join([\" \",str(config),\"=\",str(os.path.join(container_actual_data,config_value))])\n",
    "\n",
    "# 2c) Parameters command\n",
    "param_string = \"\"\n",
    "first = True\n",
    "for param in dso_container_config_algorithm[\"params\"]:\n",
    "    param_value=dso_container_config_algorithm[\"params\"][param]\n",
    "    if first:\n",
    "        param_string+=\"\".join([str(param),\"=\",str(param_value)])\n",
    "        first = False\n",
    "    else:\n",
    "        param_string+=\"\".join([\" \",str(param),\"=\",str(param_value)])\n",
    "\n",
    "# 2d) Command String\n",
    "dso_command_string = \"\".join([executable_string, \" \", config_string, \" \", param_string])\n",
    "\n",
    "\n",
    "# 3)  command string\n",
    "command_string = \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \",  dso_command_string, \"\\\"\"])\n",
    "print(\"\\nOn container \\\"{}\\\" execute the following command:  \\n\\t{} \".format(dso_container.name, command_string))\n",
    "\n",
    "# 5) Execute command\n",
    "command_res=dso_container.exec_run(command_string, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\t Done!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "\n",
    "# 6) Remove the datasets from the container\n",
    "print(\"\\nRemoving {} from {}\".format(container_actual_data,dso_container.name))\n",
    "rm_dataset_cmd = \"\".join([\"bash -c \\\"\",\"rm -r\", \" \", str(container_actual_data),\"\\\"\"])\n",
    "command_res = dso_container.exec_run(rm_dataset_cmd, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\t Done!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "\n",
    "# 7) Rename the result (a), Compress (b) and copy back to the session folder (c). (d) Finally remove results from container\n",
    "\n",
    "# (a)\n",
    "result_filename_remote_ext = \"\".join([curr_dataset, \"_\", \"DSO.txt\"])\n",
    "cmd_ren= \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \", \"mv\", \" \", dso_container_config_algorithm[\"output_file\"], \" \", result_filename_remote_ext, \"\\\"\"])\n",
    "print(\"\\nRenaming the results, using the command: {}\".format(cmd_ren))\n",
    "command_res = dso_container.exec_run(cmd_ren, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\tDone!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "# (b)\n",
    "tar_file = \"\".join([result_filename_remote_ext, \".tar.gz\"])\n",
    "cmd_cmpr_remote = \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \", \"tar -cvzf\", \" \", tar_file, \" \", result_filename_remote_ext, \"\\\"\"])\n",
    "print(\"\\ncompressing the output result command: {}\".format(cmd_cmpr_remote))\n",
    "command_res = dso_container.exec_run(cmd_cmpr_remote, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\tDone!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "# (c)\n",
    "result_cmpr_local = os.path.join(dataset_out_folder, tar_file)\n",
    "result_filename_remote_cmpr = os.path.join(dso_bin_path, tar_file)\n",
    "\n",
    "f = open(result_cmpr_local, 'wb')\n",
    "print(\"\\nTransferring the file {} ----> {}\".format(result_filename_remote_cmpr, result_cmpr_local))\n",
    "try:\n",
    "    bits, stat = dso_container.get_archive(result_filename_remote_cmpr)\n",
    "    for chunk in bits:\n",
    "        f.write(chunk)\n",
    "        f.close()\n",
    "except Exception as error:\n",
    "    print(\"\\tFatal Error: {}\".format(error))\n",
    "    sys.exit(1)\n",
    "print(\"\\tDone!\\n\\tStats:{}\".format(stat))\n",
    "\n",
    "# (d)\n",
    "result_filename_remote = os.path.join(dso_bin_path, result_filename_remote_ext)\n",
    "remove_remote_res_cmd = \"\".join([\"bash -c \\\"\", dso_ch_dir_string, \" && \", \"rm\", \" \", result_filename_remote, \" \", result_filename_remote_cmpr, \"\\\"\"])\n",
    "print(\"\\nRemoving the following file from remote container: \\t\\n- {} \\t\\n- {}\".format(result_filename_remote, result_filename_remote_cmpr))\n",
    "command_res = dso_container.exec_run(remove_remote_res_cmd, stdout=True, stderr=True, stdin=False, tty=False, privileged=False, user='', detach=False, stream=False, socket=False, environment=None, workdir=None, demux=False)\n",
    "if command_res[0]==0:\n",
    "    print(\"\\tDone!\")\n",
    "else:\n",
    "    print(\"\\tIt was not possible, error code: {}\".format(command_res))\n",
    "\n",
    "# (e) Extract file local and remove the tar file\n",
    "result_local_final = os.path.join(dataset_out_folder,result_filename_remote_ext)\n",
    "print(\"\\nDecompressing and removing {}\".format(result_cmpr_local))\n",
    "try:\n",
    "    with tarfile.open(result_cmpr_local, \"r\") as tar:\n",
    "        tar.extractall(result_local_final)\n",
    "        tar.close()\n",
    "        os.remove(result_cmpr_local)\n",
    "        print(\"\\tDone!\")\n",
    "except Exception as error:\n",
    "    print(\"\\tError while decompressing-> {}\".format(error))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transferring the file /root/Programs/dso/build/bin/Corridor_A_L_DSO.txt.tar.gz ----> /home/croc/Documenti/Repositories/evoTools/notebooks/session_29_09_2023_15_43_34/Corridor_A_L/Corridor_A_L_DSO.txt.tar.gz\n",
      "\tDone!\n",
      "\tStats:{'name': 'Corridor_A_L_DSO.txt.tar.gz', 'size': 17626, 'mode': 420, 'mtime': '2023-09-29T15:44:31.492731796+02:00', 'linkTarget': ''}\n"
     ]
    }
   ],
   "source": [
    "result_cmpr_local = os.path.join(dataset_out_folder, tar_file)\n",
    "result_filename_remote_cmpr = os.path.join(dso_bin_path, tar_file)\n",
    "\n",
    "f = open(result_cmpr_local, 'wb')\n",
    "print(\"\\nTransferring the file {} ----> {}\".format(result_filename_remote_cmpr, result_cmpr_local))\n",
    "try:\n",
    "    bits, stat = dso_container.get_archive(result_filename_remote_cmpr)\n",
    "    for chunk in bits:\n",
    "        f.write(chunk)\n",
    "        f.close()\n",
    "except Exception as error:\n",
    "    print(\"\\tFatal Error: {}\".format(error))\n",
    "    sys.exit(1)\n",
    "print(\"\\tDone!\\n\\tStats:{}\".format(stat))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decompressing and removing /home/croc/Documenti/Repositories/evoTools/notebooks/session_29_09_2023_15_43_34/Corridor_A_L/Corridor_A_L_DSO.txt.tar.gz\n",
      "\tDone!\n"
     ]
    }
   ],
   "source": [
    "result_local_final = os.path.join(dataset_out_folder,result_filename_remote_ext)\n",
    "print(\"\\nDecompressing and removing {}\".format(result_cmpr_local))\n",
    "try:\n",
    "    with tarfile.open(result_cmpr_local, \"r\") as tar:\n",
    "        tar.extractall(result_local_final)\n",
    "        tar.close()\n",
    "        os.remove(result_cmpr_local)\n",
    "        print(\"\\tDone!\")\n",
    "except Exception as error:\n",
    "    print(\"\\tError while decompressing-> {}\".format(error))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
